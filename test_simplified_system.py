#!/usr/bin/env python3
"""
ç®€åŒ–ç³»ç»Ÿæµ‹è¯• - éªŒè¯ç§»é™¤è¿›åº¦åŠŸèƒ½åç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""
import json
import tempfile
from pathlib import Path
from services.task_manager import TaskManager
from services.text_merge import TextMerger
from services.summary_generator import Summarizer

def test_task_manager():
    """æµ‹è¯•ä»»åŠ¡ç®¡ç†å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä»»åŠ¡ç®¡ç†å™¨...")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        task_manager = TaskManager(temp_dir)
        
        # åˆ›å»ºä»»åŠ¡
        task_id = task_manager.create_task("test_video.mp4")
        print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")
        
        # æ£€æŸ¥åˆå§‹çŠ¶æ€
        metadata = task_manager.load_metadata(task_id)
        assert metadata["status"] == "pending"
        assert metadata["task_id"] == task_id
        assert metadata["original_filename"] == "test_video.mp4"
        print("âœ… åˆå§‹çŠ¶æ€æ­£ç¡®")
        
        # æ›´æ–°çŠ¶æ€
        task_manager.update_status(task_id, "processing")
        metadata = task_manager.load_metadata(task_id)
        assert metadata["status"] == "processing"
        print("âœ… çŠ¶æ€æ›´æ–°æ­£ç¡®")
        
        # å®Œæˆä»»åŠ¡
        task_manager.update_status(task_id, "completed")
        metadata = task_manager.load_metadata(task_id)
        assert metadata["status"] == "completed"
        print("âœ… ä»»åŠ¡å®ŒæˆçŠ¶æ€æ­£ç¡®")
        
        # æµ‹è¯•å¤±è´¥çŠ¶æ€
        task_manager.update_status(task_id, "failed", "æµ‹è¯•é”™è¯¯")
        metadata = task_manager.load_metadata(task_id)
        assert metadata["status"] == "failed"
        assert metadata["error_message"] == "æµ‹è¯•é”™è¯¯"
        print("âœ… å¤±è´¥çŠ¶æ€æ­£ç¡®")

def test_text_merger():
    """æµ‹è¯•æ–‡æœ¬åˆå¹¶å™¨"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬åˆå¹¶å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        "result_sentences": [
            {"text": "è¿™æ˜¯ç¬¬ä¸€å¥è¯", "start_time": 1000, "end_time": 2000},
            {"text": "è¿™æ˜¯ç¬¬äºŒå¥è¯", "start_time": 2000, "end_time": 3000},
            {"text": "è¿™æ˜¯ç¬¬ä¸‰å¥è¯", "start_time": 3000, "end_time": 4000}
        ]
    }
    
    with tempfile.TemporaryDirectory() as temp_dir:
        input_file = Path(temp_dir) / "input.json"
        output_file = Path(temp_dir) / "output.json"
        
        # å†™å…¥æµ‹è¯•æ•°æ®
        with open(input_file, "w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False)
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½çœŸæ­£æµ‹è¯•
        # æˆ‘ä»¬åªæµ‹è¯•æ–‡ä»¶åŠ è½½åŠŸèƒ½
        try:
            merger = TextMerger("test-model")
            sentences = merger.load_json(str(input_file))
            assert len(sentences) == 3
            print("âœ… æ–‡æœ¬åŠ è½½æ­£ç¡®")
        except ValueError as e:
            if "ARK_API_KEY" in str(e):
                print("âš ï¸  è·³è¿‡æ–‡æœ¬åˆå¹¶æµ‹è¯• (éœ€è¦APIå¯†é’¥)")
            else:
                raise

def test_summarizer():
    """æµ‹è¯•æ‘˜è¦ç”Ÿæˆå™¨"""
    print("\nğŸ§ª æµ‹è¯•æ‘˜è¦ç”Ÿæˆå™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        "merged_sentences": [
            {
                "text": "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯æ‘˜è¦ç”ŸæˆåŠŸèƒ½ã€‚",
                "start_time": "00:00:01.000",
                "end_time": "00:00:05.000"
            }
        ]
    }
    
    with tempfile.TemporaryDirectory() as temp_dir:
        input_file = Path(temp_dir) / "input.json"
        
        # å†™å…¥æµ‹è¯•æ•°æ®
        with open(input_file, "w", encoding="utf-8") as f:
            json.dump(test_data, f, ensure_ascii=False)
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½çœŸæ­£æµ‹è¯•
        try:
            summarizer = Summarizer("test-model")
            data = summarizer.load_timed_texts(str(input_file))
            assert len(data) == 1
            print("âœ… æ‘˜è¦æ•°æ®åŠ è½½æ­£ç¡®")
        except ValueError as e:
            if "ARK_API_KEY" in str(e):
                print("âš ï¸  è·³è¿‡æ‘˜è¦ç”Ÿæˆæµ‹è¯• (éœ€è¦APIå¯†é’¥)")
            else:
                raise

def test_api_models():
    """æµ‹è¯•APIæ¨¡å‹"""
    print("\nğŸ§ª æµ‹è¯•APIæ¨¡å‹...")
    
    from models.api_models import TaskStatus, ProcessRequest
    from models.download_models import DownloadStatus
    
    # æµ‹è¯•TaskStatus
    status = TaskStatus(
        task_id="test-123",
        status="processing",
        created_at="2024-01-01T00:00:00",
        updated_at="2024-01-01T00:01:00"
    )
    assert status.task_id == "test-123"
    assert status.status == "processing"
    print("âœ… TaskStatusæ¨¡å‹æ­£ç¡®")
    
    # æµ‹è¯•ProcessRequest
    request = ProcessRequest(enable_multimodal=True, keep_temp=False)
    assert request.enable_multimodal == True
    assert request.keep_temp == False
    print("âœ… ProcessRequestæ¨¡å‹æ­£ç¡®")
    
    # æµ‹è¯•DownloadStatus
    download_status = DownloadStatus(
        task_id="test-456",
        status="completed",
        platform="youtube",
        title="æµ‹è¯•è§†é¢‘"
    )
    assert download_status.task_id == "test-456"
    assert download_status.status == "completed"
    print("âœ… DownloadStatusæ¨¡å‹æ­£ç¡®")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç®€åŒ–åçš„ç³»ç»Ÿ...")
    
    try:
        test_task_manager()
        test_text_merger()
        test_summarizer()
        test_api_models()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿç®€åŒ–æˆåŠŸï¼")
        print("\nğŸ“‹ ç®€åŒ–æ€»ç»“:")
        print("âœ… ç§»é™¤äº†æ‰€æœ‰è¿›åº¦å›è°ƒåŠŸèƒ½")
        print("âœ… ç®€åŒ–äº†ä»»åŠ¡çŠ¶æ€ç®¡ç†")
        print("âœ… æ¸…ç†äº†å‰ç«¯è¿›åº¦æ˜¾ç¤º")
        print("âœ… ä¿ç•™äº†æ ¸å¿ƒåŠŸèƒ½")
        print("âœ… ç³»ç»Ÿä»èƒ½æ­£å¸¸å·¥ä½œ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()
