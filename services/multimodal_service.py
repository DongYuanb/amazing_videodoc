"""
å¤šæ¨¡æ€æœåŠ¡ - ç»Ÿä¸€çš„å¸§æå–ã€åµŒå…¥ç”Ÿæˆå’Œå»é‡æœåŠ¡
"""
import os
import base64
import time
import logging
import tempfile
import shutil
import json
import concurrent.futures
import threading
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from pathlib import Path
import cohere
import numpy as np
from ffmpeg_process import VideoProcessor


class MultimodalService:
    """å¤šæ¨¡æ€æœåŠ¡ - ç»Ÿä¸€å¤„ç†å¸§æå–ã€åµŒå…¥ç”Ÿæˆå’Œå»é‡"""
    
    # ç±»å¸¸é‡
    DEFAULT_EMBEDDING_MODEL = "embed-v4.0"
    DEFAULT_BATCH_SIZE = 10
    DEFAULT_API_DELAY = 0.1
    SUPPORTED_IMAGE_FORMATS = {
        '.png': 'image/png',
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.gif': 'image/gif',
        '.webp': 'image/webp'
    }

    def __init__(self,
                 cohere_api_key: str,
                 ffmpeg_path: str = "ffmpeg",
                 similarity_threshold: float = 0.9,
                 embedding_model: str = DEFAULT_EMBEDDING_MODEL,
                 batch_size: int = DEFAULT_BATCH_SIZE,
                 frame_fps: float = 0.1,
                 max_concurrent_segments: int = 3,
                 logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–å¤šæ¨¡æ€æœåŠ¡
        """
        self.cohere_api_key = cohere_api_key
        self.similarity_threshold = similarity_threshold
        self.embedding_model = embedding_model
        self.batch_size = batch_size
        self.frame_fps = frame_fps
        self.max_concurrent_segments = max_concurrent_segments
        self.cohere_client = cohere.ClientV2(api_key=cohere_api_key)
        self.video_processor = VideoProcessor(ffmpeg_path)
        self.logger = logger or logging.getLogger(__name__)
        self._embedding_lock = threading.Lock()

    def extract_frames(self, video_path: str, start_time: float, end_time: float, 
                      fps: float = 1.0, output_dir: Optional[str] = None) -> List[str]:
        """ä»è§†é¢‘æŒ‡å®šæ—¶é—´èŒƒå›´æŠ½å–å¸§ - å§”æ‰˜ç»™VideoProcessor"""
        return self.video_processor.extract_frames(video_path, start_time, end_time, fps, output_dir)

    def _detect_image_content_type(self, image_path: str) -> str:
        """æ£€æµ‹å›¾ç‰‡çš„MIMEç±»å‹"""
        image_path_lower = image_path.lower()
        for ext, content_type in self.SUPPORTED_IMAGE_FORMATS.items():
            if image_path_lower.endswith(ext):
                return content_type
        return "image/jpeg"  # é»˜è®¤ä¸ºjpeg

    def _image_to_base64(self, image_path: str) -> str:
        """å°†å›¾ç‰‡è½¬æ¢ä¸ºCohereéœ€è¦çš„base64æ ¼å¼"""
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
            content_type = self._detect_image_content_type(image_path)
            return f"data:{content_type};base64,{image_data}"

    def generate_embeddings(self, image_paths: List[str],
                          batch_size: Optional[int] = None,
                          embedding_lock: Optional[object] = None) -> Tuple[List[np.ndarray], List[str]]:
        """
        æ‰¹é‡è·å–å›¾ç‰‡embeddings
        """
        if batch_size is None:
            batch_size = self.batch_size

        all_embeddings = []
        successful_paths = []

        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i + batch_size]
            self.logger.info(f"Processing batch {i//batch_size + 1}/{(len(image_paths) + batch_size - 1)//batch_size}")

            # å‡†å¤‡APIè¯·æ±‚æ•°æ®
            input_data = []
            batch_valid_paths = []

            for path in batch_paths:
                try:
                    base64_image = self._image_to_base64(path)
                    input_data.append({
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {"url": base64_image}
                            }
                        ]
                    })
                    batch_valid_paths.append(path)
                except Exception as e:
                    self.logger.warning(f"Failed to process image {path}: {e}")
                    continue

            if not input_data:
                continue

            # è°ƒç”¨Cohere APIï¼ˆä½¿ç”¨é”ä¿æŠ¤ï¼‰
            try:
                if embedding_lock:
                    with embedding_lock:
                        embeddings = self._call_cohere_api(input_data)
                        time.sleep(self.DEFAULT_API_DELAY)
                else:
                    embeddings = self._call_cohere_api(input_data)
                    time.sleep(self.DEFAULT_API_DELAY)

                all_embeddings.extend(embeddings)
                successful_paths.extend(batch_valid_paths)

            except Exception as e:
                self.logger.warning(f"API call failed for batch: {e}")
                continue

        return all_embeddings, successful_paths

    def _call_cohere_api(self, input_data: List[Dict[str, Any]]) -> List[np.ndarray]:
        """è°ƒç”¨Cohere APIè·å–embeddings"""
        try:
            response = self.cohere_client.embed(
                model=self.embedding_model,
                input_type="image",
                embedding_types=["float"],
                inputs=input_data
            )

            embeddings = []
            for embedding_data in response.embeddings.float_:
                embedding = np.array(embedding_data)
                embeddings.append(embedding)

            return embeddings

        except Exception as e:
            raise RuntimeError(f"Cohere API error: {e}")

    @staticmethod
    def calculate_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–å‘é‡
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)

        if norm1 == 0 or norm2 == 0:
            return 0.0

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
        return float(similarity)

    def remove_duplicates(self, image_paths: List[str], 
                         embeddings: List[np.ndarray]) -> List[str]:
        """
        åŸºäºembeddingsç›¸ä¼¼åº¦å»é™¤é‡å¤å›¾ç‰‡
        """
        if len(image_paths) != len(embeddings):
            raise ValueError("image_paths and embeddings must have the same length")
        
        unique_indices = []
        processed_embeddings = []
        
        for i, current_embedding in enumerate(embeddings):
            is_duplicate = False
            
            # ä¸å·²å¤„ç†çš„embeddingsæ¯”è¾ƒ
            for processed_embedding in processed_embeddings:
                similarity = self.calculate_cosine_similarity(current_embedding, processed_embedding)
                if similarity > self.similarity_threshold:
                    is_duplicate = True
                    self.logger.debug(f"Found duplicate: {image_paths[i]} (similarity: {similarity:.3f})")
                    break
            
            if not is_duplicate:
                unique_indices.append(i)
                processed_embeddings.append(current_embedding)
        
        unique_paths = [image_paths[i] for i in unique_indices]
        self.logger.info(f"Removed {len(image_paths) - len(unique_paths)} duplicate images")
        self.logger.info(f"Remaining unique images: {len(unique_paths)}")

        return unique_paths

    def save_unique_frames(self, unique_paths: List[str], output_dir: str,
                          copy_files: bool = True) -> List[str]:
        """
        ä¿å­˜å»é‡åçš„å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•
        """
        os.makedirs(output_dir, exist_ok=True)
        saved_paths = []

        for i, src_path in enumerate(unique_paths):
            # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
            filename = f"unique_frame_{i+1:06d}.jpg"
            dst_path = os.path.join(output_dir, filename)

            try:
                if copy_files:
                    shutil.copy2(src_path, dst_path)
                else:
                    shutil.move(src_path, dst_path)
                saved_paths.append(dst_path)
            except Exception as e:
                self.logger.warning(f"Failed to save {src_path}: {e}")

        self.logger.info(f"Saved {len(saved_paths)} unique frames to {output_dir}")
        return saved_paths

    def process_video_frames(self, video_path: str, start_time: float, end_time: float,
                           output_dir: str, fps: float = 1.0, temp_dir: Optional[str] = None,
                           keep_temp_files: bool = False,
                           embedding_lock: Optional[object] = None) -> Dict[str, Any]:
        """
        è§†é¢‘å¸§å»é‡å¤„ç†æµç¨‹
        """
        self.logger.info(f"Starting video frame deduplication process...")
        self.logger.info(f"Video: {video_path}")
        self.logger.info(f"Time range: {start_time}s - {end_time}s")
        self.logger.info(f"FPS: {fps}")
        self.logger.info(f"Similarity threshold: {self.similarity_threshold}")

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        if temp_dir is None:
            temp_dir = tempfile.mkdtemp(prefix="video_dedup_")
            temp_created = True
        else:
            os.makedirs(temp_dir, exist_ok=True)
            temp_created = False

        try:
            # 1. æŠ½å–å¸§
            self.logger.info("1. Extracting frames...")
            frame_paths = self.extract_frames(video_path, start_time, end_time, fps, temp_dir)

            # 2. è·å–embeddings
            self.logger.info("2. Getting embeddings...")
            embeddings, successful_paths = self.generate_embeddings(frame_paths, embedding_lock=embedding_lock)

            if len(embeddings) != len(frame_paths):
                self.logger.warning(f"Got {len(embeddings)} embeddings for {len(frame_paths)} frames")
                # ä½¿ç”¨æˆåŠŸå¤„ç†çš„å›¾ç‰‡è·¯å¾„
                frame_paths = successful_paths

            # 3. å»é™¤é‡å¤
            self.logger.info("3. Removing duplicates...")
            unique_paths = self.remove_duplicates(frame_paths, embeddings)

            # 4. ä¿å­˜ç»“æœ
            self.logger.info("4. Saving unique frames...")
            saved_paths = self.save_unique_frames(unique_paths, output_dir)

            # ç»Ÿè®¡ä¿¡æ¯
            result = {
                "video_path": video_path,
                "time_range": (start_time, end_time),
                "fps": fps,
                "total_frames": len(frame_paths),
                "unique_frames": len(saved_paths),
                "duplicates_removed": len(frame_paths) - len(saved_paths),
                "similarity_threshold": self.similarity_threshold,
                "output_dir": output_dir,
                "saved_paths": saved_paths
            }

            self.logger.info("âœ… Process completed successfully!")
            return result

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_created and not keep_temp_files:
                try:
                    shutil.rmtree(temp_dir)
                    self.logger.info(f"Cleaned up temporary directory: {temp_dir}")
                except Exception as e:
                    self.logger.warning(f"Failed to clean up temp directory: {e}")

    def _parse_time_to_seconds(self, time_str: str) -> float:
        """å°†æ—¶é—´å­—ç¬¦ä¸²(HH:MM:SS.mmm)è½¬æ¢ä¸ºç§’æ•°"""
        try:
            time_parts = time_str.split(':')
            hours = int(time_parts[0])
            minutes = int(time_parts[1])
            seconds_parts = time_parts[2].split('.')
            seconds = int(seconds_parts[0])
            milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0

            total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000
            return total_seconds
        except (ValueError, IndexError) as e:
            raise ValueError(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}, é”™è¯¯: {e}")

    def extract_segment_frames(self, video_path: str, start_time: str,
                              end_time: str, output_dir: str) -> List[str]:
        """æå–æŒ‡å®šæ—¶é—´æ®µçš„è§†é¢‘å¸§å¹¶å»é‡"""
        start_seconds = self._parse_time_to_seconds(start_time)
        end_seconds = self._parse_time_to_seconds(end_time)

        segment_dir = os.path.join(output_dir, f"segment_{start_time.replace(':', '-')}_to_{end_time.replace(':', '-')}")
        os.makedirs(segment_dir, exist_ok=True)

        try:
            result = self.process_video_frames(
                video_path=video_path,
                start_time=start_seconds,
                end_time=end_seconds,
                output_dir=segment_dir,
                fps=self.frame_fps,
                keep_temp_files=False,
                embedding_lock=self._embedding_lock
            )
            return result.get("saved_paths", [])
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ®µ {start_time}-{end_time} å¸§å¤„ç†å¤±è´¥: {e}")
            return []

    def _process_single_segment(self, segment_data: tuple) -> dict:
        """å¤„ç†å•ä¸ªæ—¶é—´æ®µçš„å¸§æå–"""
        i, segment, video_path, frames_dir, output_dir = segment_data
        start_time = segment.get("start_time", "")
        end_time = segment.get("end_time", "")
        summary = segment.get("summary", "")

        try:
            frame_paths = self.extract_segment_frames(video_path, start_time, end_time, frames_dir)
            relative_frame_paths = [os.path.relpath(path, output_dir) for path in frame_paths]
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ®µ {start_time}-{end_time} å¸§æå–å¤±è´¥: {e}")
            relative_frame_paths = []

        return {
            "segment_id": i + 1,
            "start_time": start_time,
            "end_time": end_time,
            "duration_seconds": self._parse_time_to_seconds(end_time) - self._parse_time_to_seconds(start_time),
            "summary": summary,
            "key_frames": relative_frame_paths,
            "frame_count": len(relative_frame_paths)
        }

    def generate_multimodal_notes(self, video_path: str, summary_json_path: str, output_dir: str) -> str:
        """ç”Ÿæˆå›¾æ–‡æ··æ’ç¬”è®°ï¼ˆæ”¯æŒæ—¶é—´æ®µçº§åˆ«å¹¶å‘å¤„ç†ï¼‰"""
        # è¯»å–æ‘˜è¦æ•°æ®
        with open(summary_json_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)

        summaries = summary_data.get("summaries", [])
        if not summaries:
            raise ValueError("æ‘˜è¦æ•°æ®ä¸ºç©º")

        os.makedirs(output_dir, exist_ok=True)
        frames_dir = os.path.join(output_dir, "frames")
        os.makedirs(frames_dir, exist_ok=True)

        segment_tasks = [(i, segment, video_path, frames_dir, output_dir)
                        for i, segment in enumerate(summaries)]

        multimodal_notes = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent_segments) as executor:
            future_to_index = {executor.submit(self._process_single_segment, task_data): task_data[0]
                              for task_data in segment_tasks}

            results = {}
            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    results[index] = future.result()
                except Exception as e:
                    self.logger.error(f"æ—¶é—´æ®µ {index+1} å¤„ç†å¤±è´¥: {e}")
                    segment = summaries[index]
                    results[index] = {
                        "segment_id": index + 1,
                        "start_time": segment.get("start_time", ""),
                        "end_time": segment.get("end_time", ""),
                        "duration_seconds": 0,
                        "summary": segment.get("summary", ""),
                        "key_frames": [],
                        "frame_count": 0
                    }

            for i in range(len(summaries)):
                if i in results:
                    multimodal_notes.append(results[i])

        final_notes = {
            "video_info": {
                "source_video": os.path.basename(video_path),
                "total_segments": len(multimodal_notes),
                "generated_at": datetime.now().isoformat(),
                "processing_mode": f"concurrent (max_workers={self.max_concurrent_segments})"
            },
            "segments": multimodal_notes,
            "statistics": {
                "total_frames": sum(note["frame_count"] for note in multimodal_notes),
                "segments_with_frames": len([note for note in multimodal_notes if note["frame_count"] > 0])
            }
        }

        output_file = os.path.join(output_dir, "multimodal_notes.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_notes, f, ensure_ascii=False, indent=4)

        return output_file

    def export_to_markdown(self, notes_json_path: str, output_path: str = None,
                          image_base_path: str = None) -> str:
        """å°†å›¾æ–‡ç¬”è®°å¯¼å‡ºä¸º Markdown æ ¼å¼"""
        with open(notes_json_path, 'r', encoding='utf-8') as f:
            notes_data = json.load(f)

        if output_path is None:
            output_path = f"{Path(notes_json_path).stem}.md"
        if image_base_path is None:
            image_base_path = str(Path(notes_json_path).parent)

        markdown_content = self._generate_markdown_content(notes_data, output_path, image_base_path)

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        return output_path

    def _generate_markdown_content(self, notes_data: Dict[str, Any],
                                  output_path: str = None,
                                  image_base_path: str = None) -> str:
        """ç”Ÿæˆ Markdown å†…å®¹"""
        video_info = notes_data.get("video_info", {})
        segments = notes_data.get("segments", [])
        statistics = notes_data.get("statistics", {})

        # æ„å»º Markdown
        lines = []

        # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
        lines.append(f"# ğŸ“¹ è§†é¢‘ç¬”è®°ï¼š{video_info.get('source_video', 'æœªçŸ¥è§†é¢‘')}")
        lines.append("")
        lines.append("## ğŸ“Š åŸºæœ¬ä¿¡æ¯")
        lines.append("")
        lines.append(f"- **è§†é¢‘æ–‡ä»¶**: {video_info.get('source_video', 'æœªçŸ¥')}")
        lines.append(f"- **ç”Ÿæˆæ—¶é—´**: {video_info.get('generated_at', 'æœªçŸ¥')}")
        lines.append(f"- **æ€»æ—¶é—´æ®µ**: {video_info.get('total_segments', 0)}")
        lines.append(f"- **æ€»å…³é”®å¸§**: {statistics.get('total_frames', 0)}")
        lines.append(f"- **æœ‰æ•ˆæ—¶é—´æ®µ**: {statistics.get('segments_with_frames', 0)}")
        lines.append("")

        # ç›®å½•
        lines.append("## ğŸ“‘ ç›®å½•")
        lines.append("")
        for i, segment in enumerate(segments, 1):
            start_time = segment.get("start_time", "")
            end_time = segment.get("end_time", "")
            lines.append(f"{i}. [{start_time} - {end_time}](#æ—¶é—´æ®µ-{i})")
        lines.append("")

        # è¯¦ç»†å†…å®¹
        lines.append("## ğŸ“ è¯¦ç»†å†…å®¹")
        lines.append("")

        for i, segment in enumerate(segments, 1):
            start_time = segment.get("start_time", "")
            end_time = segment.get("end_time", "")
            duration = segment.get("duration_seconds", 0)
            summary = segment.get("summary", "")
            key_frames = segment.get("key_frames", [])

            # æ—¶é—´æ®µæ ‡é¢˜
            lines.append(f"### æ—¶é—´æ®µ {i}")
            lines.append("")
            lines.append(f"**â° æ—¶é—´**: {start_time} - {end_time} ({duration:.1f}ç§’)")
            lines.append("")

            # æ‘˜è¦å†…å®¹
            lines.append("**ğŸ“‹ æ‘˜è¦**:")
            lines.append("")
            lines.append(summary)
            lines.append("")

            # å…³é”®å¸§
            if key_frames:
                lines.append(f"**ğŸ–¼ï¸ å…³é”®å¸§** ({len(key_frames)}å¼ ):")
                lines.append("")
                for frame_path in key_frames:
                    frame_name = Path(frame_path).name

                    # ç›´æ¥æ‹¼æ¥æ­£ç¡®çš„è·¯å¾„
                    if image_base_path:
                        image_path = f"/{image_base_path}/multimodal_notes/{frame_path}"
                    else:
                        image_path = f"multimodal_notes/{frame_path}"

                    lines.append(f"![{frame_name}]({image_path})")
                lines.append("")
            else:
                lines.append("*è¯¥æ—¶é—´æ®µæ— å…³é”®å¸§*")
                lines.append("")

            lines.append("---")
            lines.append("")

        # é¡µè„š
        lines.append("## ğŸ”§ ç”Ÿæˆä¿¡æ¯")
        lines.append("")
        lines.append("æœ¬ç¬”è®°ç”±è§†é¢‘å¤„ç† API è‡ªåŠ¨ç”Ÿæˆ")
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if output_path:
            lines.append(f"è¾“å‡ºæ–‡ä»¶: {output_path}")

        return "\n".join(lines)


def create_multimodal_service(cohere_api_key: str, **kwargs) -> MultimodalService:
    """ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºå¤šæ¨¡æ€æœåŠ¡"""
    return MultimodalService(cohere_api_key, **kwargs)
